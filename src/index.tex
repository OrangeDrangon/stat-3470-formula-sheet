\section{Basic Probability Rules}
\begin{align}
  P(A \cup B) = P(A) + P(B) - P(A \cap B) \\
  P(A | B) = \frac{P(A \cap B)}{P(B)} \\
  P(B) = P(A_1 \cap B) + \ldots + P(A_n \cap B) \\
  P(B) = P(A_1)P(B | A_1) + \ldots + P(A_n)P(B | A_n) \\
  P(A_n | B) = \frac{P(A_n)P(B | A_n)}{P(A_1)P(A_1 | B) + \ldots + P(A_n)P(B | A_n)}
\end{align}

\subsection{Independent}
Two events are independent IFF:
\begin{align}
  P(A | B) = P(A) \\
  P(B | A) = P(B) \\
  P(A \cap B) = P(A)P(B) 
\end{align}

\section{Discrete Random Variables}
Countably infinite or finite.

\subsection{PMF}
\begin{align}
  X = \text{random variable} \\
  x = \text{particular value of random variable $X$} \\
  pmf(x) = P(X = x) = p(x) = f(x) \\
  \forall x \in X, 0 \leq p(x) \leq 1 \\
  \sum_{i=1}^{K} p(x_i) = 1
\end{align}

\subsection{CDF}
\begin{align}
  F(x) = P(X \leq x) \text{defined for $x$} \in \mathbb{R} \\
  F(x) \geq 0 \\
  F(x) \text{is increasing} \\
  \lim_{x\to-\infty} F(x) = 0 \\
  \lim_{x\to\infty} F(x) = 1
\end{align}

\subsection{Expected Values}
\begin{align}
  E(X) = \sum_{i}^{} x_ip(x_i) = M_X \\
  E(aX + b) = aE(X) + b \\
  E(h(X)) = \sum_{i}^{} h(x_i)p(x_i)
\end{align}

\subsection{Variance}
\begin{align}
  V(X) = \sigma^2_X \\
  V(X) = \sum_{i}^{}(x_i - M_x)^2p(x_i) \\
  V(X) = E(X^2) - E(X)^2 \\
  V(aX + b) = a^2V(X)
\end{align}

\subsection{Standard Deviation}
\begin{align}
  SD(X) = \sigma_X \\
  SD(X) = \sqrt{V(X)} \\
  SD(aX + b) = \lvert a \rvert SD(X)
\end{align}

\subsection{Bernoulli}
$X$ takes a value of $1$ with $p$ and $0$ with $1-p$.
\begin{align}
  X \sim Bern(x) \\
  E(X) = p \\
  V(X) = p(1 - p)
\end{align}

\subsection{Binomial}
Binary, independent, and fixed number of trials with a fixed probability of success. Sum of independent Bernoulli random trials. $X$ is the number of successes.
\begin{align}
  X \sim Binom(n, p) \\
  p(x) = \binom{n}{x}p^x(1 - p)^{n - x} \\
  E(X) = np \\
  V(X) = np(1 - p)
\end{align}

\subsection{Geometric}
Independent Bernoulli trials until the first success. $X$ is the number of trials until the first success.
\begin{align}
  X \sim Geom(p) \\
  f(x) = (1 - p)^{x - 1}p \\
  E(X) = \frac{1}{p} \\
  V(X) = \frac{1 - p}{p^2}
\end{align}

\subsection{Negative Binomial}
$X$ is the number of trials until the $k^{\text{th}}$ success occurs.
\begin{align}
  X \sim NegBinom(k, p) \\
  f(x) = \binom{x - 1}{k - 1}p^k(1 - p)^{x-k} \\
  E(X) = \frac{k}{p} \\
  V(X) = \frac{k(1 - p)}{p^2}
\end{align}

\subsection{Poisson}
Counts the number of times something occurs in a given interval of time or space.
\begin{align}
  X \sim Poisson(M) \\
  f(x) = \frac{e^{-M}M^X}{x!} \\
  E(X) = M \\
  V(X) = M
\end{align}

\section{Continuous Random Variables}

\subsection{PDF}
\begin{align}
  \forall x \in \mathbb{R}, f(x) \geq 0 \\
  \int_{-\infty}^{\infty} f(x) dx = 1 \\
  P(a \leq x \leq b) = \int_{a}^{b} f(x) dx
\end{align}

\subsection{CDF}
\begin{align}
  F(x) = P(X \leq x) \\
  F(x) = \int_{-\infty}^{x} f(x') dx'
\end{align}

\subsection{Percentiles}
Given by $\eta(p)$.
\begin{align}
  F(\eta(p)) = P
\end{align}

\subsection{Expected Value}
\begin{align}
  E(X) = \int_{-\infty}^{\infty} xf(x) dx \\
  E(h(X)) = \int_{-\infty}^{\infty} h(x)f(x) dx
\end{align}

\subsection{Variance}
\begin{align}
  V(X) = \int_{-\infty}^{\infty} (x - M_X)^2f(x) dx \\
  V(X) = E(X^2) - E(X)^2
\end{align}

\subsection{Uniform}
\begin{align}
  X \sim Uni(a, b) \\
  f(x) = \begin{cases}
    0 & x \leq a \\
    \frac{1}{b - a} & a < x < b \\
    0 & x \geq b
  \end{cases} \\
  F(x) = \begin{cases}
    0 & x \leq a \\
    \frac{x - a}{b - a} & a < x < b \\
    1 & x \geq b
  \end{cases} \\
  E(X) = \frac{a + b}{2} \\
  V(X) = \frac{(b - a)^2}{12}
\end{align}

\subsection{Normal}
\begin{align}
  X \sim N(M, \sigma^2) \\
  f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2}(\frac{x - M}{\sigma})^2} \\
  F(x) = \int_{-\infty}^{x} f(x') dx' \\
  E(X) = M \\
  V(X) = \sigma^2 \\
  aX + b \sim N(aM + b, a^2\sigma^2)
\end{align}

\subsubsection{Standard Normal}
\begin{align}
  Z \sim N(0, 1) \\
  Z = \frac{x - M}{\sigma} \sim N(0, 1)
\end{align}

\subsection{Exponential}
\begin{align}
  X \sim Exp(\lambda) \\
  f(x) = \begin{cases}
    0 & x \leq 0 \\
    \lambda e^{-\lambda x} & x > 0
  \end{cases} \\
  F(x) = 1 - e^{-\lambda x} \\
  E(X) = \frac{1}{\lambda} \\
  V(X) = \frac{1}{\lambda^2} \\
  P(X \geq x) = e^{-\lambda x}
\end{align}

\subsubsection{Relation to Poisson}
Distribution of time between successive events is $Exp(\alpha)$.
\begin{align}
  X \sim Poisson(\alpha t) \\
  Y \sim Exp(\alpha)
\end{align}

\subsection{Gamma}
\begin{align}
  X \sim Gamma(\alpha, \beta) \\
  f(x) = \frac{1}{\beta^{\alpha}\Gamma(\alpha)}x^{\alpha - 1}e^{-\frac{x}{\beta}} \\
  E(X) = \alpha\beta \\
  V(X) = \alpha\beta^2
\end{align}

\section{Combining Random Variables}
\subsection{PMF}
\subsubsection{Discrete}
\begin{align}
  pmf(x, y) = P(X = x, Y = y) \\
  \forall x \in X, 0 \leq p(x) \leq 1 \\
  \sum_{i, j}^{} p(x_i, y_j) = 1 \\
  f_x(x) = \sum_{j}^{} f(x, y_j)
\end{align}

\subsubsection{Continuous}
\begin{align}
  \forall x \in \mathbb{R}, \forall y \in \mathbb{R}, f(x, y) \geq 0 \\
  \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x, y) dx dy = 1 \\
  f_x(x) = \int_{-\infty}^{\infty} f(x, y) dy
\end{align}

\subsection{Conditional Probability}
\begin{align}
  f(x | y) = P(X = x | Y = y) = \frac{P(X = x, Y = y)}{P(Y = y)} = \frac{f(x, y)}{f_y(y)}
\end{align}

\subsection{Independence}
2 random variables are independent IFF
\begin{align}
  f(x | y) = f_x(x) \\
  f(y | x) = f_y(y) \\
  f(x, y) = f_x(x) * f_y(y)
\end{align}

\subsection{Expected Value}
\begin{align}
  E(h(X, Y)) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} h(x, y) f(x, y) dx dy
\end{align}

\subsection{Covariance and Correlation}
How the two random variables vary together. Positive means that as $X$ gets bigger $Y$ gets bigger and negative means as $X$ gets bigger $Y$ gets smaller. If $X$ and $Y$ are independent they will have a correlation of $0$. No correlation does not imply independence though. 
\begin{align}
  Cov(X, Y) = E(X - M_x)E(Y - M_y) \\
  Cov(X, Y) = E(XY) - E(X)E(Y) \\
  \rho = Corr(X, Y) = \frac{Cov(X, Y)}{\sigma_x\sigma_y}
\end{align}

\subsection{Sampling of Sample Mean}
\begin{align}
  \overbar{X} \sim N(M, \frac{\sigma^2}{n}) \\
  E(\overbar{X}) = E(X) \\
  V(\overbar{X}) = \frac{V(X)}{n}
\end{align}

\subsubsection{Central Limit Theorem}
Can approximate a distribution as normal when sampled with $n \geq 30$.

\subsection{Sampling of Sample Proportion}
Can approximate $\hat{p}$ when $np \geq 10$ and $n(1 - p) \geq 10$.
\begin{align}
  \hat{p} = \frac{\text{successes}}{\text{trials}} \\
  E(X) = p \\
  Var(X) = \frac{p(1-p)}{n}
\end{align}

\subsection{Effect of Combination on Expected Value and Variance}
\begin{align}
  E(a_1X_1 + \ldots) = E(a_1X_1) + \ldots \\
  V(\sum_{i=1}^{n} a_iX_i) = \sum_{i=1}^{n} \sum_{j=1}^{n} a_ia_jCov(x_i, x_j)
\end{align}

\subsubsection{Special Case}
\begin{align}
  E(X - Y) = E(X) - E(Y) \\
  V(X - Y) = V(X) + V(Y) - 2 * Cov(X, Y)
\end{align}

\section{Estimation}
\subsection{Bias}
\begin{align}
  Bias(\hat{\theta}) = E(\hat{\theta}) - \theta \\
  Unbiased \iff E(\hat{\theta}) = \theta \\
  MSE(\hat{\theta}) = E((\hat{\theta} - \theta)^2) = Var(\hat{\theta}) + Bias(\hat{\theta})^2
\end{align}

\subsection{Method of Moments (MOM)}
\begin{enumerate}
\item Find expressions for the first $k$ population moments. $E(X), E(X^2), \cdots, E(X^k)$
\item Set those expressions equal to the first $k$ sample momments.
  \begin{align}
    E(X) = \overbar{x} \\
    E(X^2) = \overbar{x^2} \\
    \cdots
  \end{align}
\item Solve the system of $k$ equations and unknowns.
\item Hattify all the $\theta$s to get estimates called $\hat{\theta}_{MOM}$
\end{enumerate}

\subsubsection{Useful Notes}
\begin{align}
  E(X^2) = V(X^2) + E(X)^2 \\
  \overbar{x^2} - \overbar{x}^2 = \frac{n - 1}{n}s^2
\end{align}

\subsection{Maximum Likelihood (MLE)}
\begin{enumerate}
\item Find $L(\theta) = \prod_{i=1}^n f_{x_i}(X_i; \theta_1, \theta_2, \cdots, \theta_k)$
\item Take the $\ln$ of $L(\theta)$ to get log likelihood $l(\theta)$
\item Take $\diff{l(\theta)}{\theta}$
\item Solve $\diff{l(\theta)}{\theta} = 0$ for $\theta$
\item Hattify $\theta$ to get $\hat{\theta}_{MLE}$
\item Verify the second derivative is negative to ensure it is a local maxima.
\end{enumerate}

\subsubsection{Invariance}
If $h$ is a well defined function of $\theta$ then $h(\theta) = h(\theta_{MLE})$

\section{Confidence Intervals}
\subsection{$\mu$; $\sigma$ known}
\begin{enumerate}
\item (Simple) random sample
\item X is normal or $n \geq 30$
\end{enumerate}

\begin{align}
  (ME) = Z\frac{\sigma}{\sqrt{n}} \\
  \overbar{x} \pm Z\frac{\sigma}{\sqrt{n}} \\
  n = \frac{Z^2\sigma^2}{(ME)^2} \\
  \text{ROUND UP}
\end{align}

\subsection{P}
\begin{enumerate}
\item (Simple) random sample
\item $n\hat{p} \geq 10$ and $ n(1 - \hat{p}) \geq 10$
\end{enumerate}

\begin{align}
  ME = Z\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \\
  \overbar{x} \pm Z\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \\
  n = \frac{Z^2\hat{p}(1 - \hat{p})}{(ME)^2} \\
  \text{ROUND UP}
\end{align}

\subsubsection{Unknown $\hat{p}$}
\begin{enumerate}
\item Guess using previous data
\item Use worst case of $\hat{p} = 0.5$ (Conservative Approach)'
\end{enumerate}

\subsection{$\mu$; Unknown $\sigma$}
\begin{enumerate}
\item (Simple) random sample
\item X is normal or n is large enough to approximate normal. NOTE: we often looke for symmetrical data to say this is roughly met as this often needs to be applied with a small sample size.
\end{enumerate}

\begin{align}
  df = n - 1 \\
  t = \frac{x - \overbar{x}}{s} \\
  ME = t\frac{s}{\sqrt{n}}
\end{align}

\subsection{Prediction interval}
\begin{enumerate}
\item (Simple) random sample
\item X comes from a normal distribution
\end{enumerate}

\begin{align}
  \overbar{x} \pm t * s\sqrt{1 + \frac{1}{n}}
\end{align}

\section{Hypothesis Tests}
Can only ever reject the null never prove the alternative.

\subsection{$\mu$; Known $\sigma$}
\begin{enumerate}
\item Write $H_0$ and $H_a$ and define parameters.
\item Check assumptions.
  \begin{enumerate}
  \item (Simple) Random sample
  \item $\sigma$ is known
  \item X is normal or $n \geq 30$
  \end{enumerate}
\item Calculate test statistic and compute p-value.
  \begin{align}
    Z_{obs} = \frac{\overbar{x} - \mu_0}{\frac{\sigma}{\sqrt{n}}} \\
    H_a : \mu > \mu_0 \text{ right tail } P(Z > Z_{obs}) \\
    H_a : \mu < \mu_0 \text{ left tail } P(Z < Z_{obs}) \\
    H_a : \mu \ne \mu_0 \text{ both tails } 2*P(Z > \lvert Z_{obs}\rvert)
  \end{align}
\item Comapre p-value to $\alpha$. If $\alpha$ not provided use $\alpha = 0.05$
\item Write conclution in context
\end{enumerate}

\subsection{P}
\begin{enumerate}
\item Write $H_0$ and $H_a$ and define parameters.
\item Check assumptions.
  \begin{enumerate}
  \item (Simple) Random sample
  \item n is large enough. $np_0 \geq 10$ and $ n(1 - p_0) \geq 10$
  \end{enumerate}
\item Calculate test statistic and compute p-value.
  \begin{align}
    Z_{obs} = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}}
  \end{align}
\item Comapre p-value to $\alpha$. If $\alpha$ not provided use $\alpha = 0.05$
\item Write conclution in context
\end{enumerate}

\subsection{$\mu$; Unknown $\sigma$}
\begin{enumerate}
\item Write $H_0$ and $H_a$ and define parameters.
\item Check assumptions.
  \begin{enumerate}
  \item (Simple) Random sample
  \item $\sigma$ is not known
  \item X is normal or $n \geq 30$. NOTE: may need to use graph of data to check reasonably symmetric data if n is to small as this is often when this is applied.
  \end{enumerate}
\item Calculate test statistic and compute p-value.
  \begin{align}
    t_{obs} = \frac{\overbar{x} - \mu_0}{\frac{s}{\sqrt{n}}}
  \end{align}
\item Comapre p-value to $\alpha$. If $\alpha$ not provided use $\alpha = 0.05$
\item Write conclution in context
\end{enumerate}

\subsection{Errors and Power}
\begin{center}
  \begin{tabular}{|c|c|c|}
    \hline
    & Reject $H_0$ & Do Not Reject $H_0$ \\
    \hline
    $H_0$ true & type I & \\
    \hline
    $H_0$ false & power & type II \\
    \hline
  \end{tabular}
\end{center}

\begin{align}
  type I = \alpha
\end{align}

\subsubsection{Calculating Type II}
\begin{enumerate}
\item Calculate cuttoff using percentiles.
\item Calculate probability of getting cuttoff value using the provided true mean.
  \begin{enumerate}
  \item Less than becomes greater than
  \item Greater than becomes less than
  \item Not equal becomes between
  \end{enumerate}
\end{enumerate}
